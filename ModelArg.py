import io
import os
import math
import torch
import warnings
from itertools import chain
from dataclasses import dataclass
from tqdm.notebook import tqdm
from collections.abc import Mapping
from datasets import load_dataset
from torch.utils.data.dataset import Dataset
from transformers.data.data_collator import DataCollatorMixin
from transformers import (CONFIG_MAPPING,
                          MODEL_FOR_MASKED_LM_MAPPING,
                          PreTrainedTokenizer,
                          TrainingArguments,
                          AutoConfig,
                          AutoTokenizer,
                          AutoModelForMaskedLM,
                          DataCollatorForLanguageModeling,
                          DataCollatorForWholeWordMask,
                          PretrainedConfig,
                          Trainer,
                          set_seed)
from config import output_model_dir, model_type, dataset_name, vocab_size, max_seq_length, mlm_probability, whole_word_mask, line_by_line, pad_to_max_length
# Set seed for reproducibility,
set_seed(69)
#chay lai notebook cung ket qua

# Look for gpu to use. Will use `cpu` by default if no gpu found.
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)

class ModelDataArguments(object):
    r"""Arguments pertaining to which model/config/tokenizer/data we are going to fine-tune, or train.

    Eve though all arguments are optional, there still needs to be a certain
    number of arguments that require values attributed.

    Raises:

            ValueError: If `CONFIG_MAPPING` is not loaded in global variables.

            ValueError: If `model_type` is not present in `CONFIG_MAPPING.keys()`.

            ValueError: If `model_type`, `model_config_name` and
            `model_name_or_path` variables are all `None`. At least one of them
            needs to be set.

            warnings: If `model_config_name` and `model_name_or_path` are both
            `None`, the model will be trained from scratch.
            
            model_type: Lo·∫°i m√¥ h√¨nh s·ª≠ d·ª•ng: bert, roberta
        
            config_name: C·∫•u h√¨nh c·ªßa m√¥ h√¨nh s·ª≠ d·ª•ng: bert, roberta
            
            tokenizer_name: Tokenizer ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu cho vi·ªác hu·∫•n luy·ªán m√¥ h√¨nh.
            Th√¥ng th∆∞·ªùng, n√≥ c√≥ c√πng t√™n v·ªõi model_name_or_path: bert-base-cased, roberta-base, v.v.
            
            model_name_or_path :ƒê∆∞·ªùng d·∫´n ƒë·∫øn m√¥ h√¨nh transformer hi·ªán c√≥ 
            ho·∫∑c t√™n c·ªßa m√¥ h√¨nh transformer s·∫Ω ƒë∆∞·ª£c s·ª≠ d·ª•ng: bert-base-cased, roberta-base vv.
            
            dataset_name: T√™n c·ªßa b·ªô d·ªØ li·ªáu s·ª≠ d·ª•ng (qua th∆∞ vi·ªán datasets).
            
            dataset_config_name: T√™n c·∫•u h√¨nh c·ªßa b·ªô d·ªØ li·ªáu s·∫Ω s·ª≠ d·ª•ng (qua th∆∞ vi·ªán datasets). 
            
            cache_dir: ƒê∆∞·ªùng d·∫´n ƒë·∫øn c√°c t·ªáp cache. N√≥ gi√∫p ti·∫øt ki·ªám th·ªùi gian khi ch·∫°y l·∫°i code. 
            
            preprocessing_num_workers: S·ªë l∆∞·ª£ng "qu√° tr√¨nh" s·ª≠ d·ª•ng cho qu√° tr√¨nh ti·ªÅn x·ª≠ l√Ω
            
            line_by_line: C√≥ n√™n x·ª≠ l√Ω c√°c d√≤ng vƒÉn b·∫£n kh√°c nhau trong t·∫≠p d·ªØ li·ªáu nh∆∞ l√† c√°c chu·ªói kh√°c nhau kh√¥ng?
            whole_word_mask: ƒê∆∞·ª£c s·ª≠ d·ª•ng nh∆∞ l√† flag ƒë·ªÉ x√°c ƒë·ªãnh li·ªáu ch√∫ng ta c√≥ quy·∫øt ƒë·ªãnh s·ª≠ d·ª•ng vi·ªác che k√≠n to√†n b·ªô t·ª´ hay kh√¥ng. Vi·ªác che k√≠n to√†n b·ªô t·ª´ c√≥ nghƒ©a l√† to√†n b·ªô t·ª´ s·∫Ω ƒë∆∞·ª£c che k√≠n
            trong qu√° tr√¨nh ƒë√†o t·∫°o thay v√¨ c√°c token c√≥ th·ªÉ l√† c√°c ph·∫ßn c·ªßa t·ª´.
            
            mlm_probability: Khi ƒë√†o t·∫°o c√°c m√¥ h√¨nh ng√¥n ng·ªØ b·ªã mask . C·∫ßn ph·∫£i c√≥ mlm=True. 
            N√≥ bi·ªÉu th·ªã x√°c su·∫•t che khu·∫•t c√°c token khi ƒë√†o t·∫°o m√¥ h√¨nh.
            
            max_seq_length: ƒê·ªô d√†i t·ªëi ƒëa c·ªßa chu·ªói ƒë·∫ßu v√†o sau khi ƒë∆∞·ª£c m√£ h√≥a th√†nh c√°c token. C√°c chu·ªói d√†i h∆°n s·∫Ω b·ªã c·∫Øt ng·∫Øn.
            
            pad_to_max_length: C√≥ n√™n ƒë·ªám t·∫•t c·∫£ c√°c m·∫´u cho ƒë·∫øn max_seq_length kh√¥ng? N·∫øu False, 
            pad  s·∫Ω ƒë∆∞·ª£c ƒë·ªám theo c√°ch linh ho·∫°t khi chia th√†nh batch sao cho ƒë·∫°t ƒë·∫øn ƒë·ªô d√†i t·ªëi ƒëa trong batch.
            overwrite_cache:N·∫øu c√≥ b·∫•t k·ª≥ t·ªáp ƒë∆∞·ª£c l∆∞u tr·ªØ trong b·ªô nh·ªõ cache, h√£y ghi ƒë√® l√™n ch√∫ng.
            validation_split_percentage:  T·ª∑ l·ªá ph·∫ßn trƒÉm c·ªßa t·∫≠p hu·∫•n luy·ªán ƒë∆∞·ª£c s·ª≠ d·ª•ng l√†m t·∫≠p validation n·∫øu kh√¥ng c√≥ ph√¢n chia validation.
    """

    def __init__(self,
               model_type=None, 
               config_name=None,
               tokenizer_name=None,
               model_name_or_path=None,
               dataset_name=None,
               dataset_config_name=None,
               cache_dir=None,
               preprocessing_num_workers=None,
               line_by_line=False,
               whole_word_mask=False,
               mlm_probability=0.15,
               max_seq_length=-1,
               pad_to_max_length=False,
               overwrite_cache=False,
               validation_split_percentage=5,
               ):

    # Make sure CONFIG_MAPPING is imported from transformers module.
    ## ƒê·∫£m b·∫£o r·∫±ng CONFIG_MAPPING ƒë∆∞·ª£c nh·∫≠p t·ª´ m√¥-ƒëun mtransformers.

        if 'CONFIG_MAPPING' not in globals():
            raise ValueError('Could not find CONFIG_MAPPING imported! ' \
                            'Make sure to import it from `transformers` module!')

        # Make sure model_type is valid.
        if (model_type is not None) and (model_type not in CONFIG_MAPPING.keys()):
            raise ValueError('Invalid `model_type`! Use one of the following: %s' %
                            (str(list(CONFIG_MAPPING.keys()))))

        # Make sure that model_type, config_name and model_name_or_path
        # variables are not all `None`.
        if not any([model_type, config_name, model_name_or_path]):
            raise ValueError('You can`t have all `model_type`, `config_name`,' \
                            ' `model_name_or_path` be `None`! You need to have' \
                            'at least one of them set!')

        # Check if a new model will be loaded from scratch.
        if not any([config_name, model_name_or_path]):
        # Setup warning to show pretty. This is an overkill
            warnings.formatwarning = lambda message,category,*args,**kwargs: \
                                    '%s: %s\n' % (category.__name__, message)
            # Display warning.
            warnings.warn('You are planning to train a model from scratch! üôÄ')

            # Set all data related arguments.
            self.dataset_name = dataset_name
            self.dataset_config_name = dataset_config_name
            self.preprocessing_num_workers = preprocessing_num_workers
            self.line_by_line = line_by_line
            self.whole_word_mask = whole_word_mask
            self.mlm_probability = mlm_probability
            self.max_seq_length = max_seq_length
            self.pad_to_max_length = pad_to_max_length
            self.overwrite_cache = overwrite_cache
            self.validation_split_percentage = validation_split_percentage

            # Set all model and tokenizer arguments.
            self.model_type = model_type
            self.config_name = config_name
            self.tokenizer_name = tokenizer_name
            self.model_name_or_path = model_name_or_path
            self.cache_dir = cache_dir
    
    
    def get_model_config(self, override_config):
        r"""
        Get model configuration.

        Using the ModelDataArguments return the model configuration.

        Arguments:

            args (:obj:`ModelDataArguments`):
            Model and data configuration arguments needed to perform pretraining.

            override_config (:obj:`Config`):
            Configuration to replace the old one.

        Returns:

            :obj:`PretrainedConfig`: Model transformers configuration.

        """

        # Check model configuration.
        if self.config_name is not None:
            # Use model configure name if defined.
            model_config = AutoConfig.from_pretrained(self.config_name,
                                            cache_dir=self.cache_dir)

        elif self.model_name_or_path is not None:
            # Use model name or path if defined.
            model_config = AutoConfig.from_pretrained(self.model_name_or_path,
                                            cache_dir=self.cache_dir)

        else:
            # Use config mapping if building model from scratch.
            model_config = CONFIG_MAPPING[self.model_type]()

        if override_config:
            model_config.update(override_config)

        return model_config
    def get_model(self, model_config):
        """
        Get model.

        Using the ModelDataArguments return the actual model.

        Arguments:
            model_config (:obj:`PretrainedConfig`): Model transformers configuration.

        Returns:
            :obj:`torch.nn.Module`: PyTorch model.
        """

        if 'MODEL_FOR_MASKED_LM_MAPPING' not in globals():
            raise ValueError('Could not find MODEL_FOR_MASKED_LM_MAPPING is imported!')

        if self.model_name_or_path:
            if type(model_config) in MODEL_FOR_MASKED_LM_MAPPING.keys():
                return AutoModelForMaskedLM.from_pretrained(
                    self.model_name_or_path,
                    from_tf=bool(".ckpt" in self.model_name_or_path),
                    config=model_config,
                    cache_dir=self.cache_dir,
                )
            else:
                raise ValueError(
                    'Invalid `model_name_or_path`! It should be in %s!' % str(MODEL_FOR_MASKED_LM_MAPPING.keys())
                )

        else:
            print("Training new model from scratch!")
            return AutoModelForMaskedLM.from_config(model_config)
    def get_tokenizer(self, local_path, config):
        r"""
        Get model tokenizer.

        Using the ModelDataArguments return the model tokenizer and change
        `max_seq_length` from `args` if needed.

        Arguments:

            args (:obj:`ModelDataArguments`):
            Model and data configuration arguments needed to perform pre-training.

            local_path (:obj:`str`):
            Path to the trained tokenizer.

            config (:obj:`Config`):
            Model Configuration.

        Returns:

            :obj:`PreTrainedTokenizer`: Model transformers tokenizer.

        """

        # Check tokenizer configuration.
        if self.tokenizer_name:
            # Use tokenizer name if defined.
            tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name,
                                                    cache_dir=self.cache_dir)

        elif self.model_name_or_path:
            # Use tokenizer name of path if defined.
            tokenizer = AutoTokenizer.from_pretrained(self.model_name_or_path,
                                                    cache_dir=self.cache_dir)
        else:
            tokenizer = AutoTokenizer.from_pretrained(local_path,
                                                    config=config,
                                                    cache_dir=self.cache_dir)
        
        # Setup data maximum number of tokens.
        if self.max_seq_length <= 0:
            # Set max_seq_length to maximum length of tokenizer.
            # Input max_seq_length will be the max possible for the model.
            self.max_seq_length = tokenizer.model_max_length
        else:
            # Never go beyond tokenizer maximum length.
            self.max_seq_length = min(self.max_seq_length, tokenizer.model_max_length)

        return tokenizer




model_data_args = ModelDataArguments(
    dataset_name=dataset_name,
    line_by_line=line_by_line,
    whole_word_mask=whole_word_mask,
    mlm_probability=mlm_probability,
    max_seq_length=max_seq_length,
    pad_to_max_length=pad_to_max_length,
    overwrite_cache=False,
    model_type=model_type,
    cache_dir=None
)

