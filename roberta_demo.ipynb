{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import warnings\n",
    "from itertools import chain\n",
    "from dataclasses import dataclass\n",
    "from tqdm.notebook import tqdm\n",
    "from collections.abc import Mapping\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from transformers.data.data_collator import DataCollatorMixin\n",
    "from transformers import (CONFIG_MAPPING,\n",
    "                          MODEL_FOR_MASKED_LM_MAPPING,\n",
    "                          PreTrainedTokenizer,\n",
    "                          TrainingArguments,\n",
    "                          AutoConfig,\n",
    "                          AutoTokenizer,\n",
    "                          AutoModelForMaskedLM,\n",
    "                          DataCollatorForLanguageModeling,\n",
    "                          DataCollatorForWholeWordMask,\n",
    "                          PretrainedConfig,\n",
    "                          Trainer,\n",
    "                          set_seed)\n",
    "\n",
    "# Set seed for reproducibility,\n",
    "set_seed(69)\n",
    "#chay lai notebook cung ket qua\n",
    "\n",
    "# Look for gpu to use. Will use `cpu` by default if no gpu found.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Giờ đu công, chui thùng lạnh thôi. Đi XKLĐ cũn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TẬP CẬN BÌNH QUA VIỆT NAM THĂM LÀ ĐIỀU RẤT TỐT...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Có phải “ chiên anton “ là thằng hôm trước về ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hay quá 🌷💙Buổi nói chuyện của 2 vị rất hay. Xi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sao không đòi công lý cho nạn nhân bị nhiễm ch...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>quân trương  phân tích  lịch sử rất sâu să...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Chxhcn việt nam (trừ tự do, trừ hạnh phúc)😂😂😂</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ngu vậy sao giàu được</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Đây chắc là cơ hội cho đang dân chủ thằng chiế...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Vin cũng biết tận dụng bán hàng</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1,000 NGƯỜI BỊ CẢNH SÁT MỸ BẮN CHẾT MỖI NĂM CÒ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TT. BIden. Buot phai hannh dongg truoc khi nuo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Chống ngập đường nói 20 năm rồi mà vẫn ngập nă...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Ế thì phải bỏ ra làm taxi, để thành nghĩa địa ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>@ngocannguyen8049  VNCH bị Bắc Cộng lừa ký và...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Câu nói để đoi của Ô Phúc \"cây cột điện ở Mỹ c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>😂 có mà cho tiền chúng cũng không dám</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Là con người không ai lại vô liêm sỹ tự làm th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Một biện pháp nghiệp vụ được đào tạo rất kỹ  r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Ong  ho im di may muon lấmo</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Lúc nào cũng dan do thái và dct, nên tìm hiểu ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>@thanhracroi7177  \\nThứ sáu là có chiến thắng...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Đánh cho Mỹ cút đánh cho ngụy nhào thống nhất ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3/// vên vàng ko muốn xem video này 😂😂😂</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Cộng đồng chung vận mệnh từ đẹp này Tập Cận Bì...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Dạ phải. “Bác” đã dạy: “Đảng viên đi trước, là...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Đừng nói ngô kỷ hết nước chấm không phải bêđê ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Giao Sim Tại Nhà -  Vào Tên Chính Chủ\\nKiểm Tr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Hội đồng bảo an Hiệp Chúng Quốc</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[TB] Tặng 20% giá trị tất cả thẻ nạp trong ngà...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              content  label\n",
       "0   Giờ đu công, chui thùng lạnh thôi. Đi XKLĐ cũn...      0\n",
       "1   TẬP CẬN BÌNH QUA VIỆT NAM THĂM LÀ ĐIỀU RẤT TỐT...      0\n",
       "2   Có phải “ chiên anton “ là thằng hôm trước về ...      0\n",
       "3   Hay quá 🌷💙Buổi nói chuyện của 2 vị rất hay. Xi...      0\n",
       "4   Sao không đòi công lý cho nạn nhân bị nhiễm ch...      0\n",
       "5   quân trương  phân tích  lịch sử rất sâu să...      0\n",
       "6       Chxhcn việt nam (trừ tự do, trừ hạnh phúc)😂😂😂      0\n",
       "7                               Ngu vậy sao giàu được      0\n",
       "8   Đây chắc là cơ hội cho đang dân chủ thằng chiế...      0\n",
       "9                     Vin cũng biết tận dụng bán hàng      0\n",
       "10  1,000 NGƯỜI BỊ CẢNH SÁT MỸ BẮN CHẾT MỖI NĂM CÒ...      0\n",
       "11  TT. BIden. Buot phai hannh dongg truoc khi nuo...      0\n",
       "12  Chống ngập đường nói 20 năm rồi mà vẫn ngập nă...      0\n",
       "13  Ế thì phải bỏ ra làm taxi, để thành nghĩa địa ...      0\n",
       "14   @ngocannguyen8049  VNCH bị Bắc Cộng lừa ký và...      0\n",
       "15  Câu nói để đoi của Ô Phúc \"cây cột điện ở Mỹ c...      0\n",
       "16              😂 có mà cho tiền chúng cũng không dám      0\n",
       "17  Là con người không ai lại vô liêm sỹ tự làm th...      0\n",
       "18  Một biện pháp nghiệp vụ được đào tạo rất kỹ  r...      0\n",
       "19                        Ong  ho im di may muon lấmo      0\n",
       "20  Lúc nào cũng dan do thái và dct, nên tìm hiểu ...      0\n",
       "21   @thanhracroi7177  \\nThứ sáu là có chiến thắng...      0\n",
       "22  Đánh cho Mỹ cút đánh cho ngụy nhào thống nhất ...      0\n",
       "23            3/// vên vàng ko muốn xem video này 😂😂😂      0\n",
       "24  Cộng đồng chung vận mệnh từ đẹp này Tập Cận Bì...      0\n",
       "25  Dạ phải. “Bác” đã dạy: “Đảng viên đi trước, là...      0\n",
       "26  Đừng nói ngô kỷ hết nước chấm không phải bêđê ...      0\n",
       "27  Giao Sim Tại Nhà -  Vào Tên Chính Chủ\\nKiểm Tr...      1\n",
       "28                    Hội đồng bảo an Hiệp Chúng Quốc      0\n",
       "29  [TB] Tặng 20% giá trị tất cả thẻ nạp trong ngà...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset to examine its structure\n",
    "data_path = 'shuffled_quarter_final.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "data.head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "with open('vietnamese-stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "        black_words = f.read().splitlines()\n",
    "\n",
    "def remove_control_and_normalize(text):\n",
    "\n",
    "    text = ''.join(char for char in text if not unicodedata.combining(char))\n",
    "\n",
    "    text = re.sub(r'(https?://[^\\s]+)', 'URL', text)\n",
    "\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = text.replace('/', ' ')\n",
    "    text = text.replace('(', ' ')\n",
    "    text = text.replace(')', ' ')\n",
    "    text = re.sub(r'\\b\\w*\\d\\w*\\b', '', text)  # Loại bỏ từ có số\n",
    "\n",
    "    text = re.sub(r'[^\\w\\s]', '', text, flags=re.UNICODE)\n",
    "\n",
    "    # text = text.lower()\n",
    "\n",
    "\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "\n",
    "    # text = \" \".join(word for word in text.split() if word not in black_words)\n",
    "\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>cleaned_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Giờ đu công, chui thùng lạnh thôi. Đi XKLĐ cũn...</td>\n",
       "      <td>Giờ đu công chui thùng lạnh thôi Đi XKLĐ cũng ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TẬP CẬN BÌNH QUA VIỆT NAM THĂM LÀ ĐIỀU RẤT TỐT...</td>\n",
       "      <td>TẬP CẬN BÌNH QUA VIỆT NAM THĂM LÀ ĐIỀU RẤT TỐT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Có phải “ chiên anton “ là thằng hôm trước về ...</td>\n",
       "      <td>Có phải chiên anton là thằng hôm trước về VN l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hay quá 🌷💙Buổi nói chuyện của 2 vị rất hay. Xi...</td>\n",
       "      <td>Hay quá Buổi nói chuyện của vị rất hay Xin cảm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sao không đòi công lý cho nạn nhân bị nhiễm ch...</td>\n",
       "      <td>Sao không đòi công lý cho nạn nhân bị nhiễm ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>quân trương  phân tích  lịch sử rất sâu să...</td>\n",
       "      <td>quân trương phân tich lich sư rât sâu săc dê hiêu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Chxhcn việt nam (trừ tự do, trừ hạnh phúc)😂😂😂</td>\n",
       "      <td>Chxhcn việt nam trừ tự do trừ hạnh phúc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ngu vậy sao giàu được</td>\n",
       "      <td>Ngu vậy sao giàu được</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Đây chắc là cơ hội cho đang dân chủ thằng chiế...</td>\n",
       "      <td>Đây chắc là cơ hội cho đang dân chủ thằng chiế...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Vin cũng biết tận dụng bán hàng</td>\n",
       "      <td>Vin cũng biết tận dụng bán hàng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1,000 NGƯỜI BỊ CẢNH SÁT MỸ BẮN CHẾT MỖI NĂM CÒ...</td>\n",
       "      <td>NGƯỜI BỊ CẢNH SÁT MỸ BẮN CHẾT MỖI NĂM CÒN KHÔN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TT. BIden. Buot phai hannh dongg truoc khi nuo...</td>\n",
       "      <td>TT BIden Buot phai hannh dongg truoc khi nuoc ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Chống ngập đường nói 20 năm rồi mà vẫn ngập nă...</td>\n",
       "      <td>Chống ngập đường nói năm rồi mà vẫn ngập năm n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Ế thì phải bỏ ra làm taxi, để thành nghĩa địa ...</td>\n",
       "      <td>Ế thì phải bỏ ra làm taxi để thành nghĩa địa x...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>@ngocannguyen8049  VNCH bị Bắc Cộng lừa ký và...</td>\n",
       "      <td>VNCH bị Bắc Cộng lừa ký vào bản hiệp định Pari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Câu nói để đoi của Ô Phúc \"cây cột điện ở Mỹ c...</td>\n",
       "      <td>Câu nói để đoi của Ô Phúc cây cột điện ở Mỹ cũ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>😂 có mà cho tiền chúng cũng không dám</td>\n",
       "      <td>có mà cho tiền chúng cũng không dám</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Là con người không ai lại vô liêm sỹ tự làm th...</td>\n",
       "      <td>Là con người không ai lại vô liêm sỹ tự làm th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Một biện pháp nghiệp vụ được đào tạo rất kỹ  r...</td>\n",
       "      <td>Một biện pháp nghiệp vụ được đào tạo rất kỹ rồ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Ong  ho im di may muon lấmo</td>\n",
       "      <td>Ong ho im di may muon lấmo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Lúc nào cũng dan do thái và dct, nên tìm hiểu ...</td>\n",
       "      <td>Lúc nào cũng dan do thái và dct nên tìm hiểu n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>@thanhracroi7177  \\nThứ sáu là có chiến thắng...</td>\n",
       "      <td>Thứ sáu là có chiến thắng rực rỡ như ngày</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Đánh cho Mỹ cút đánh cho ngụy nhào thống nhất ...</td>\n",
       "      <td>Đánh cho Mỹ cút đánh cho ngụy nhào thống nhất ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3/// vên vàng ko muốn xem video này 😂😂😂</td>\n",
       "      <td>vên vàng ko muốn xem video này</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Cộng đồng chung vận mệnh từ đẹp này Tập Cận Bì...</td>\n",
       "      <td>Cộng đồng chung vận mệnh từ đẹp này Tập Cận Bì...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Dạ phải. “Bác” đã dạy: “Đảng viên đi trước, là...</td>\n",
       "      <td>Dạ phải Bác đã dạy Đảng viên đi trước làng nướ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Đừng nói ngô kỷ hết nước chấm không phải bêđê ...</td>\n",
       "      <td>Đừng nói ngô kỷ hết nước chấm không phải bêđê ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Giao Sim Tại Nhà -  Vào Tên Chính Chủ\\nKiểm Tr...</td>\n",
       "      <td>Giao Sim Tại Nhà Vào Tên Chính ChủnKiểm Tra Đú...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Hội đồng bảo an Hiệp Chúng Quốc</td>\n",
       "      <td>Hội đồng bảo an Hiệp Chúng Quốc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[TB] Tặng 20% giá trị tất cả thẻ nạp trong ngà...</td>\n",
       "      <td>TB Tặng giá trị tất cả thẻ nạp trong ngày Tiền...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              content  \\\n",
       "0   Giờ đu công, chui thùng lạnh thôi. Đi XKLĐ cũn...   \n",
       "1   TẬP CẬN BÌNH QUA VIỆT NAM THĂM LÀ ĐIỀU RẤT TỐT...   \n",
       "2   Có phải “ chiên anton “ là thằng hôm trước về ...   \n",
       "3   Hay quá 🌷💙Buổi nói chuyện của 2 vị rất hay. Xi...   \n",
       "4   Sao không đòi công lý cho nạn nhân bị nhiễm ch...   \n",
       "5   quân trương  phân tích  lịch sử rất sâu să...   \n",
       "6       Chxhcn việt nam (trừ tự do, trừ hạnh phúc)😂😂😂   \n",
       "7                               Ngu vậy sao giàu được   \n",
       "8   Đây chắc là cơ hội cho đang dân chủ thằng chiế...   \n",
       "9                     Vin cũng biết tận dụng bán hàng   \n",
       "10  1,000 NGƯỜI BỊ CẢNH SÁT MỸ BẮN CHẾT MỖI NĂM CÒ...   \n",
       "11  TT. BIden. Buot phai hannh dongg truoc khi nuo...   \n",
       "12  Chống ngập đường nói 20 năm rồi mà vẫn ngập nă...   \n",
       "13  Ế thì phải bỏ ra làm taxi, để thành nghĩa địa ...   \n",
       "14   @ngocannguyen8049  VNCH bị Bắc Cộng lừa ký và...   \n",
       "15  Câu nói để đoi của Ô Phúc \"cây cột điện ở Mỹ c...   \n",
       "16              😂 có mà cho tiền chúng cũng không dám   \n",
       "17  Là con người không ai lại vô liêm sỹ tự làm th...   \n",
       "18  Một biện pháp nghiệp vụ được đào tạo rất kỹ  r...   \n",
       "19                        Ong  ho im di may muon lấmo   \n",
       "20  Lúc nào cũng dan do thái và dct, nên tìm hiểu ...   \n",
       "21   @thanhracroi7177  \\nThứ sáu là có chiến thắng...   \n",
       "22  Đánh cho Mỹ cút đánh cho ngụy nhào thống nhất ...   \n",
       "23            3/// vên vàng ko muốn xem video này 😂😂😂   \n",
       "24  Cộng đồng chung vận mệnh từ đẹp này Tập Cận Bì...   \n",
       "25  Dạ phải. “Bác” đã dạy: “Đảng viên đi trước, là...   \n",
       "26  Đừng nói ngô kỷ hết nước chấm không phải bêđê ...   \n",
       "27  Giao Sim Tại Nhà -  Vào Tên Chính Chủ\\nKiểm Tr...   \n",
       "28                    Hội đồng bảo an Hiệp Chúng Quốc   \n",
       "29  [TB] Tặng 20% giá trị tất cả thẻ nạp trong ngà...   \n",
       "\n",
       "                                      cleaned_content  \n",
       "0   Giờ đu công chui thùng lạnh thôi Đi XKLĐ cũng ...  \n",
       "1   TẬP CẬN BÌNH QUA VIỆT NAM THĂM LÀ ĐIỀU RẤT TỐT...  \n",
       "2   Có phải chiên anton là thằng hôm trước về VN l...  \n",
       "3   Hay quá Buổi nói chuyện của vị rất hay Xin cảm...  \n",
       "4   Sao không đòi công lý cho nạn nhân bị nhiễm ch...  \n",
       "5   quân trương phân tich lich sư rât sâu săc dê hiêu  \n",
       "6             Chxhcn việt nam trừ tự do trừ hạnh phúc  \n",
       "7                               Ngu vậy sao giàu được  \n",
       "8   Đây chắc là cơ hội cho đang dân chủ thằng chiế...  \n",
       "9                     Vin cũng biết tận dụng bán hàng  \n",
       "10  NGƯỜI BỊ CẢNH SÁT MỸ BẮN CHẾT MỖI NĂM CÒN KHÔN...  \n",
       "11  TT BIden Buot phai hannh dongg truoc khi nuoc ...  \n",
       "12  Chống ngập đường nói năm rồi mà vẫn ngập năm n...  \n",
       "13  Ế thì phải bỏ ra làm taxi để thành nghĩa địa x...  \n",
       "14  VNCH bị Bắc Cộng lừa ký vào bản hiệp định Pari...  \n",
       "15  Câu nói để đoi của Ô Phúc cây cột điện ở Mỹ cũ...  \n",
       "16                có mà cho tiền chúng cũng không dám  \n",
       "17  Là con người không ai lại vô liêm sỹ tự làm th...  \n",
       "18  Một biện pháp nghiệp vụ được đào tạo rất kỹ rồ...  \n",
       "19                         Ong ho im di may muon lấmo  \n",
       "20  Lúc nào cũng dan do thái và dct nên tìm hiểu n...  \n",
       "21          Thứ sáu là có chiến thắng rực rỡ như ngày  \n",
       "22  Đánh cho Mỹ cút đánh cho ngụy nhào thống nhất ...  \n",
       "23                     vên vàng ko muốn xem video này  \n",
       "24  Cộng đồng chung vận mệnh từ đẹp này Tập Cận Bì...  \n",
       "25  Dạ phải Bác đã dạy Đảng viên đi trước làng nướ...  \n",
       "26  Đừng nói ngô kỷ hết nước chấm không phải bêđê ...  \n",
       "27  Giao Sim Tại Nhà Vào Tên Chính ChủnKiểm Tra Đú...  \n",
       "28                    Hội đồng bảo an Hiệp Chúng Quốc  \n",
       "29  TB Tặng giá trị tất cả thẻ nạp trong ngày Tiền...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['cleaned_content'] = data['content'].apply(remove_control_and_normalize)\n",
    "\n",
    "# Hiển thị một số dòng đầu để kiểm tra kết quả\n",
    "data[['content', 'cleaned_content']].head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import models, pre_tokenizers, decoders, trainers, processors, Tokenizer, ByteLevelBPETokenizer\n",
    "\n",
    "# Initialize a tokenizer with BPE model\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "\n",
    "# Define vocabulary size for BPE\n",
    "vocab_size = 30000\n",
    "\n",
    "# Configure the BPE trainer\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=vocab_size, \n",
    "    min_frequency=2, \n",
    "    special_tokens=[\n",
    "        \"<s>\",  # Start of Sentence\n",
    "        \"<pad>\",  # Padding\n",
    "        \"</s>\",  # End of Sentence\n",
    "        \"<unk>\",  # Unknown Token\n",
    "        \"<mask>\",  # Masking Token\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Train the tokenizer using the cleaned content\n",
    "cleaned_content = data['cleaned_content']\n",
    "tokenizer.train_from_iterator(iterator=cleaned_content, trainer=trainer)\n",
    "tokenizer.save(\"tokenizer.json\")\n",
    "\n",
    "# Set up a RoBERTa-style post-processor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'T', 'ao', 'ĠlÃł', 'Ġbá»ĳ', 'ĠmÃły', '</s>']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.post_processor = processors.RobertaProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "\n",
    "# Enable truncation to a maximum length of 512 tokens\n",
    "tokenizer.enable_truncation(max_length=512)\n",
    "\n",
    "# Save or return the tokenizer for further use\n",
    "\n",
    "encoding = tokenizer.encode(\"Tao là bố mày!\")\n",
    "encoding.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tao là bố mày'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "tokenizer.decode(encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element 1471 is invalid: None\n",
      "Element 1894 is invalid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: cleaned_content. If cleaned_content are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "c:\\Users\\Admin\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4797\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1800\n",
      "  Number of trainable parameters = 41209146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized content: [5775, 1209, 425, 2435, 4403, 2993, 468, 312, 204, 42, 43, 661, 324, 3567, 2539, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Tokenized content: [51, 466, 3322, 680, 420, 664, 511, 2092, 187, 896, 457, 758, 1204, 2447, 721, 507, 457, 846, 866, 664, 511, 3537, 1201, 3564, 587, 420, 664, 511, 721, 232, 231, 420, 260, 5994, 893, 1818, 420, 664, 511, 468, 613, 411, 231, 1023, 2192, 284, 461, 794, 2491, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Tokenized content: [2816, 334, 3871, 639, 3804, 187, 538, 1234, 698, 386, 560, 284, 1869, 516, 218, 370, 2017, 356, 498, 7480, 2134, 416, 6035, 559, 231, 723, 2929, 330, 906, 1482, 472, 1229, 187, 680, 869, 5124, 370, 6038, 115, 3226, 1725, 333, 5843, 562, 431, 521, 356, 1498, 260, 757, 512, 260, 351, 644, 2065, 456, 235, 7258, 56, 2309, 618, 803, 2000, 1584, 3448, 364, 7599, 2303, 305, 535, 1157, 829, 172, 1062, 490, 305, 1139, 163, 49, 4179, 1224, 187, 423, 216, 296, 279, 5118, 4613, 3892, 560, 412, 4203, 378, 1418, 336, 468, 476, 435, 4609, 618, 1693, 1890, 362, 218, 1016, 279, 383, 372, 174, 252, 2511, 219, 60, 45, 520, 3056, 1413, 433, 235, 893, 2419, 1426, 8161, 364, 1327, 1730, 427, 2196, 245, 272]\n",
      "Tokenized content: [5777, 445, 2929, 316, 791, 245, 935, 457, 443, 792, 908, 605, 935, 590, 429, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Tokenized content: [2822, 231, 1437, 425, 820, 238, 1193, 477, 370, 3019, 995, 974, 1195, 1121, 245, 587, 456, 664, 511, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff8ed2a9de5049af9358d2e97ed68f1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 94\u001b[0m\n\u001b[0;32m     86\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     87\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     88\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m     89\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_dataset,\n\u001b[0;32m     90\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[0;32m     91\u001b[0m )\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Thực hiện pre-training mô hình\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:1501\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m   1498\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1499\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1500\u001b[0m )\n\u001b[1;32m-> 1501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:1749\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1747\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1748\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1749\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1752\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1753\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1754\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1755\u001b[0m ):\n\u001b[0;32m   1756\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1757\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:2508\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   2507\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2508\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   2511\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:2540\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2539\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2540\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   2541\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2542\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2543\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1107\u001b[0m, in \u001b[0;36mRobertaForMaskedLM.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1093\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroberta(\n\u001b[0;32m   1094\u001b[0m     input_ids,\n\u001b[0;32m   1095\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1104\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1105\u001b[0m )\n\u001b[0;32m   1106\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1107\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1109\u001b[0m masked_lm_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1144\u001b[0m, in \u001b[0;36mRobertaLMHead.forward\u001b[1;34m(self, features, **kwargs)\u001b[0m\n\u001b[0;32m   1141\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(x)\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;66;03m# project back to size of vocabulary with bias\u001b[39;00m\n\u001b[1;32m-> 1144\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import thư viện cần thiết\n",
    "from transformers import RobertaConfig, RobertaModel,RobertaForMaskedLM, RobertaTokenizerFast, DataCollatorForLanguageModeling, Trainer, TrainingArguments, AutoTokenizer, CONFIG_MAPPING\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Thiết lập cấu hình mô hình với thông số tùy chỉnh\n",
    "model_type = 'roberta'  # Loại mô hình\n",
    "model_config = CONFIG_MAPPING[model_type]()  # Cấu hình mặc định cho mô hình\n",
    "override_config = {\n",
    "    \"num_hidden_layers\": 8,\n",
    "    \"num_attention_heads\": 8,\n",
    "    \"intermediate_size\": 2048,\n",
    "    \"hidden_size\": 512,\n",
    "    \"max_position_embeddings\": 130,  # 128 cho nội dung + 2 cho token đặc biệt\n",
    "}\n",
    "model_config.update(override_config)  # Cập nhật cấu hình tùy chỉnh\n",
    "\n",
    "# Khởi tạo mô hình RoBERTa với cấu hình tùy chỉnh\n",
    "model = RobertaForMaskedLM(model_config)\n",
    "\n",
    "# Sử dụng tokenizer đã huấn luyện từ tệp hoặc thư mục\n",
    "tokenizer_path = \"C:\\\\Users\\\\Admin\\\\Downloads\\\\roberta\\\\mlm_bert_2\"  # Đường dẫn đến thư mục chứa tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "# Sử dụng bộ dữ liệu CSV đã cho\n",
    "dataset_path = \"cleaned_quarter_final.csv\"\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": dataset_path})\n",
    "train_dataset = dataset[\"train\"]\n",
    "for i in range(len(train_dataset)):\n",
    "    content = train_dataset[i].get(\"cleaned_content\", None)\n",
    "    \n",
    "    if content is None or not isinstance(content, str):\n",
    "        print(f\"Element {i} is invalid: {content}\")  # Ghi lại phần tử lỗi\n",
    "\n",
    "# Hàm kiểm tra và chuyển đổi dữ liệu thành chuỗi\n",
    "# Hàm để đảm bảo mọi giá trị là chuỗi và trả về danh sách\n",
    "# for i in range(999 ,1005 ):\n",
    "#     content = train_dataset[i].get(\"cleaned_content\", \"\")\n",
    "#     if content is None:\n",
    "#         print(f\"Data {i} is None or empty.\")\n",
    "# Kiểm tra sau khi lọc\n",
    "# Lọc các phần tử có giá trị None trong \"cleaned_content\"\n",
    "valid_dataset = train_dataset.filter(lambda example: example.get(\"cleaned_content\") is not None)\n",
    "# Loại bỏ cột label\n",
    "dataset_without_label = valid_dataset.remove_columns(\"label\") \n",
    "# Hoặc tên cột liên quan đến nhãn\n",
    "dataset_without_label = dataset_without_label.remove_columns(\"content\")\n",
    "# Tokenize dữ liệu và thiết lập Data Collator cho Language Modeling\n",
    "def tokenize_function(example):\n",
    "    content = example.get(\"cleaned_content\", \"\")\n",
    "    \n",
    "    # Nếu là None hoặc không phải là chuỗi, chuyển thành chuỗi\n",
    "    if content is None or not isinstance(content, str):\n",
    "        content = \"\"\n",
    "\n",
    "    return tokenizer(example[\"cleaned_content\"], truncation=True, max_length=130, padding=\"max_length\")\n",
    "\n",
    "tokenized_dataset = dataset_without_label.map(tokenize_function, batched=True)\n",
    "# Lấy 5 phần tử đầu tiên\n",
    "# Lấy 5 phần tử đầu tiên từ tập dữ liệu\n",
    "first_5_examples = tokenized_dataset.select(range(5))\n",
    "\n",
    "for example in first_5_examples:\n",
    "    print(\"Tokenized content:\", example[\"input_ids\"])  # Kiểm tra nội dung được token hóa\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,  # Masked Language Modeling\n",
    "    mlm_probability=0.15  # Tỷ lệ phần trăm các từ được mask\n",
    ")\n",
    "\n",
    "# Thiết lập các tham số huấn luyện\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=500,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500,\n",
    ")\n",
    "\n",
    "# Thiết lập Trainer và thực hiện huấn luyện\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Thực hiện pre-training mô hình\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
