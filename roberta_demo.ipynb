{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import warnings\n",
    "from itertools import chain\n",
    "from dataclasses import dataclass\n",
    "from tqdm.notebook import tqdm\n",
    "from collections.abc import Mapping\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from transformers.data.data_collator import DataCollatorMixin\n",
    "from transformers import (CONFIG_MAPPING,\n",
    "                          MODEL_FOR_MASKED_LM_MAPPING,\n",
    "                          PreTrainedTokenizer,\n",
    "                          TrainingArguments,\n",
    "                          AutoConfig,\n",
    "                          AutoTokenizer,\n",
    "                          AutoModelForMaskedLM,\n",
    "                          DataCollatorForLanguageModeling,\n",
    "                          DataCollatorForWholeWordMask,\n",
    "                          PretrainedConfig,\n",
    "                          Trainer,\n",
    "                          set_seed)\n",
    "\n",
    "# Set seed for reproducibility,\n",
    "set_seed(69)\n",
    "#chay lai notebook cung ket qua\n",
    "\n",
    "# Look for gpu to use. Will use `cpu` by default if no gpu found.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Giá» Ä‘u cÃ´ng, chui thÃ¹ng láº¡nh thÃ´i. Äi XKLÄ cÅ©n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Táº¬P Cáº¬N BÃŒNH QUA VIá»†T NAM THÄ‚M LÃ€ ÄIá»€U Ráº¤T Tá»T...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CÃ³ pháº£i â€œ chiÃªn anton â€œ lÃ  tháº±ng hÃ´m trÆ°á»›c vá» ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hay quÃ¡ ğŸŒ·ğŸ’™Buá»•i nÃ³i chuyá»‡n cá»§a 2 vá»‹ ráº¥t hay. Xi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sao khÃ´ng Ä‘Ã²i cÃ´ng lÃ½ cho náº¡n nhÃ¢n bá»‹ nhiá»…m ch...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>quÃ¢n trÆ°Æ¡ng  phÃ¢n tiÌch  liÌ£ch sÆ°Ì‰ rÃ¢Ìt sÃ¢u sÄƒ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Chxhcn viá»‡t nam (trá»« tá»± do, trá»« háº¡nh phÃºc)ğŸ˜‚ğŸ˜‚ğŸ˜‚</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ngu váº­y sao giÃ u Ä‘Æ°á»£c</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ÄÃ¢y cháº¯c lÃ  cÆ¡ há»™i cho Ä‘ang dÃ¢n chá»§ tháº±ng chiáº¿...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Vin cÅ©ng biáº¿t táº­n dá»¥ng bÃ¡n hÃ ng</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1,000 NGÆ¯á»œI Bá»Š Cáº¢NH SÃT Má»¸ Báº®N CHáº¾T Má»–I NÄ‚M CÃ’...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TT. BIden. Buot phai hannh dongg truoc khi nuo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Chá»‘ng ngáº­p Ä‘Æ°á»ng nÃ³i 20 nÄƒm rá»“i mÃ  váº«n ngáº­p nÄƒ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>áº¾ thÃ¬ pháº£i bá» ra lÃ m taxi, Ä‘á»ƒ thÃ nh nghÄ©a Ä‘á»‹a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>@ngocannguyen8049Â  VNCH bá»‹ Báº¯c Cá»™ng lá»«a kÃ½ vÃ ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CÃ¢u nÃ³i Ä‘á»ƒ Ä‘oi cá»§a Ã” PhÃºc \"cÃ¢y cá»™t Ä‘iá»‡n á»Ÿ Má»¹ c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ğŸ˜‚ cÃ³ mÃ  cho tiá»n chÃºng cÅ©ng khÃ´ng dÃ¡m</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LÃ  con ngÆ°á»i khÃ´ng ai láº¡i vÃ´ liÃªm sá»¹ tá»± lÃ m th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Má»™t biá»‡n phÃ¡p nghiá»‡p vá»¥ Ä‘Æ°á»£c Ä‘Ã o táº¡o ráº¥t ká»¹  r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Ong  ho im di may muon láº¥mo</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LÃºc nÃ o cÅ©ng dan do thÃ¡i vÃ  dct, nÃªn tÃ¬m hiá»ƒu ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>@thanhracroi7177Â  \\nThá»© sÃ¡u lÃ  cÃ³ chiáº¿n tháº¯ng...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ÄÃ¡nh cho Má»¹ cÃºt Ä‘Ã¡nh cho ngá»¥y nhÃ o thá»‘ng nháº¥t ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3/// vÃªn vÃ ng ko muá»‘n xem video nÃ y ğŸ˜‚ğŸ˜‚ğŸ˜‚</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Cá»™ng Ä‘á»“ng chung váº­n má»‡nh tá»« Ä‘áº¹p nÃ y Táº­p Cáº­n BÃ¬...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Dáº¡ pháº£i. â€œBÃ¡câ€ Ä‘Ã£ dáº¡y: â€œÄáº£ng viÃªn Ä‘i trÆ°á»›c, lÃ ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Äá»«ng nÃ³i ngÃ´ ká»· háº¿t nÆ°á»›c cháº¥m khÃ´ng pháº£i bÃªÄ‘Ãª ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Giao Sim Táº¡i NhÃ  -  VÃ o TÃªn ChÃ­nh Chá»§\\nKiá»ƒm Tr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Há»™i Ä‘á»“ng báº£o an Hiá»‡p ChÃºng Quá»‘c</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[TB] Táº·ng 20% giÃ¡ trá»‹ táº¥t cáº£ tháº» náº¡p trong ngÃ ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              content  label\n",
       "0   Giá» Ä‘u cÃ´ng, chui thÃ¹ng láº¡nh thÃ´i. Äi XKLÄ cÅ©n...      0\n",
       "1   Táº¬P Cáº¬N BÃŒNH QUA VIá»†T NAM THÄ‚M LÃ€ ÄIá»€U Ráº¤T Tá»T...      0\n",
       "2   CÃ³ pháº£i â€œ chiÃªn anton â€œ lÃ  tháº±ng hÃ´m trÆ°á»›c vá» ...      0\n",
       "3   Hay quÃ¡ ğŸŒ·ğŸ’™Buá»•i nÃ³i chuyá»‡n cá»§a 2 vá»‹ ráº¥t hay. Xi...      0\n",
       "4   Sao khÃ´ng Ä‘Ã²i cÃ´ng lÃ½ cho náº¡n nhÃ¢n bá»‹ nhiá»…m ch...      0\n",
       "5   quÃ¢n trÆ°Æ¡ng  phÃ¢n tiÌch  liÌ£ch sÆ°Ì‰ rÃ¢Ìt sÃ¢u sÄƒ...      0\n",
       "6       Chxhcn viá»‡t nam (trá»« tá»± do, trá»« háº¡nh phÃºc)ğŸ˜‚ğŸ˜‚ğŸ˜‚      0\n",
       "7                               Ngu váº­y sao giÃ u Ä‘Æ°á»£c      0\n",
       "8   ÄÃ¢y cháº¯c lÃ  cÆ¡ há»™i cho Ä‘ang dÃ¢n chá»§ tháº±ng chiáº¿...      0\n",
       "9                     Vin cÅ©ng biáº¿t táº­n dá»¥ng bÃ¡n hÃ ng      0\n",
       "10  1,000 NGÆ¯á»œI Bá»Š Cáº¢NH SÃT Má»¸ Báº®N CHáº¾T Má»–I NÄ‚M CÃ’...      0\n",
       "11  TT. BIden. Buot phai hannh dongg truoc khi nuo...      0\n",
       "12  Chá»‘ng ngáº­p Ä‘Æ°á»ng nÃ³i 20 nÄƒm rá»“i mÃ  váº«n ngáº­p nÄƒ...      0\n",
       "13  áº¾ thÃ¬ pháº£i bá» ra lÃ m taxi, Ä‘á»ƒ thÃ nh nghÄ©a Ä‘á»‹a ...      0\n",
       "14  Â @ngocannguyen8049Â  VNCH bá»‹ Báº¯c Cá»™ng lá»«a kÃ½ vÃ ...      0\n",
       "15  CÃ¢u nÃ³i Ä‘á»ƒ Ä‘oi cá»§a Ã” PhÃºc \"cÃ¢y cá»™t Ä‘iá»‡n á»Ÿ Má»¹ c...      0\n",
       "16              ğŸ˜‚ cÃ³ mÃ  cho tiá»n chÃºng cÅ©ng khÃ´ng dÃ¡m      0\n",
       "17  LÃ  con ngÆ°á»i khÃ´ng ai láº¡i vÃ´ liÃªm sá»¹ tá»± lÃ m th...      0\n",
       "18  Má»™t biá»‡n phÃ¡p nghiá»‡p vá»¥ Ä‘Æ°á»£c Ä‘Ã o táº¡o ráº¥t ká»¹  r...      0\n",
       "19                        Ong  ho im di may muon láº¥mo      0\n",
       "20  LÃºc nÃ o cÅ©ng dan do thÃ¡i vÃ  dct, nÃªn tÃ¬m hiá»ƒu ...      0\n",
       "21  Â @thanhracroi7177Â  \\nThá»© sÃ¡u lÃ  cÃ³ chiáº¿n tháº¯ng...      0\n",
       "22  ÄÃ¡nh cho Má»¹ cÃºt Ä‘Ã¡nh cho ngá»¥y nhÃ o thá»‘ng nháº¥t ...      0\n",
       "23            3/// vÃªn vÃ ng ko muá»‘n xem video nÃ y ğŸ˜‚ğŸ˜‚ğŸ˜‚      0\n",
       "24  Cá»™ng Ä‘á»“ng chung váº­n má»‡nh tá»« Ä‘áº¹p nÃ y Táº­p Cáº­n BÃ¬...      0\n",
       "25  Dáº¡ pháº£i. â€œBÃ¡câ€ Ä‘Ã£ dáº¡y: â€œÄáº£ng viÃªn Ä‘i trÆ°á»›c, lÃ ...      0\n",
       "26  Äá»«ng nÃ³i ngÃ´ ká»· háº¿t nÆ°á»›c cháº¥m khÃ´ng pháº£i bÃªÄ‘Ãª ...      0\n",
       "27  Giao Sim Táº¡i NhÃ  -  VÃ o TÃªn ChÃ­nh Chá»§\\nKiá»ƒm Tr...      1\n",
       "28                    Há»™i Ä‘á»“ng báº£o an Hiá»‡p ChÃºng Quá»‘c      0\n",
       "29  [TB] Táº·ng 20% giÃ¡ trá»‹ táº¥t cáº£ tháº» náº¡p trong ngÃ ...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset to examine its structure\n",
    "data_path = 'shuffled_quarter_final.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "data.head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "with open('vietnamese-stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "        black_words = f.read().splitlines()\n",
    "\n",
    "def remove_control_and_normalize(text):\n",
    "\n",
    "    text = ''.join(char for char in text if not unicodedata.combining(char))\n",
    "\n",
    "    text = re.sub(r'(https?://[^\\s]+)', 'URL', text)\n",
    "\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = text.replace('/', ' ')\n",
    "    text = text.replace('(', ' ')\n",
    "    text = text.replace(')', ' ')\n",
    "    text = re.sub(r'\\b\\w*\\d\\w*\\b', '', text)  # Loáº¡i bá» tá»« cÃ³ sá»‘\n",
    "\n",
    "    text = re.sub(r'[^\\w\\s]', '', text, flags=re.UNICODE)\n",
    "\n",
    "    # text = text.lower()\n",
    "\n",
    "\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "\n",
    "    # text = \" \".join(word for word in text.split() if word not in black_words)\n",
    "\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>cleaned_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Giá» Ä‘u cÃ´ng, chui thÃ¹ng láº¡nh thÃ´i. Äi XKLÄ cÅ©n...</td>\n",
       "      <td>Giá» Ä‘u cÃ´ng chui thÃ¹ng láº¡nh thÃ´i Äi XKLÄ cÅ©ng ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Táº¬P Cáº¬N BÃŒNH QUA VIá»†T NAM THÄ‚M LÃ€ ÄIá»€U Ráº¤T Tá»T...</td>\n",
       "      <td>Táº¬P Cáº¬N BÃŒNH QUA VIá»†T NAM THÄ‚M LÃ€ ÄIá»€U Ráº¤T Tá»T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CÃ³ pháº£i â€œ chiÃªn anton â€œ lÃ  tháº±ng hÃ´m trÆ°á»›c vá» ...</td>\n",
       "      <td>CÃ³ pháº£i chiÃªn anton lÃ  tháº±ng hÃ´m trÆ°á»›c vá» VN l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hay quÃ¡ ğŸŒ·ğŸ’™Buá»•i nÃ³i chuyá»‡n cá»§a 2 vá»‹ ráº¥t hay. Xi...</td>\n",
       "      <td>Hay quÃ¡ Buá»•i nÃ³i chuyá»‡n cá»§a vá»‹ ráº¥t hay Xin cáº£m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sao khÃ´ng Ä‘Ã²i cÃ´ng lÃ½ cho náº¡n nhÃ¢n bá»‹ nhiá»…m ch...</td>\n",
       "      <td>Sao khÃ´ng Ä‘Ã²i cÃ´ng lÃ½ cho náº¡n nhÃ¢n bá»‹ nhiá»…m ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>quÃ¢n trÆ°Æ¡ng  phÃ¢n tiÌch  liÌ£ch sÆ°Ì‰ rÃ¢Ìt sÃ¢u sÄƒ...</td>\n",
       "      <td>quÃ¢n trÆ°Æ¡ng phÃ¢n tich lich sÆ° rÃ¢t sÃ¢u sÄƒc dÃª hiÃªu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Chxhcn viá»‡t nam (trá»« tá»± do, trá»« háº¡nh phÃºc)ğŸ˜‚ğŸ˜‚ğŸ˜‚</td>\n",
       "      <td>Chxhcn viá»‡t nam trá»« tá»± do trá»« háº¡nh phÃºc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ngu váº­y sao giÃ u Ä‘Æ°á»£c</td>\n",
       "      <td>Ngu váº­y sao giÃ u Ä‘Æ°á»£c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ÄÃ¢y cháº¯c lÃ  cÆ¡ há»™i cho Ä‘ang dÃ¢n chá»§ tháº±ng chiáº¿...</td>\n",
       "      <td>ÄÃ¢y cháº¯c lÃ  cÆ¡ há»™i cho Ä‘ang dÃ¢n chá»§ tháº±ng chiáº¿...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Vin cÅ©ng biáº¿t táº­n dá»¥ng bÃ¡n hÃ ng</td>\n",
       "      <td>Vin cÅ©ng biáº¿t táº­n dá»¥ng bÃ¡n hÃ ng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1,000 NGÆ¯á»œI Bá»Š Cáº¢NH SÃT Má»¸ Báº®N CHáº¾T Má»–I NÄ‚M CÃ’...</td>\n",
       "      <td>NGÆ¯á»œI Bá»Š Cáº¢NH SÃT Má»¸ Báº®N CHáº¾T Má»–I NÄ‚M CÃ’N KHÃ”N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TT. BIden. Buot phai hannh dongg truoc khi nuo...</td>\n",
       "      <td>TT BIden Buot phai hannh dongg truoc khi nuoc ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Chá»‘ng ngáº­p Ä‘Æ°á»ng nÃ³i 20 nÄƒm rá»“i mÃ  váº«n ngáº­p nÄƒ...</td>\n",
       "      <td>Chá»‘ng ngáº­p Ä‘Æ°á»ng nÃ³i nÄƒm rá»“i mÃ  váº«n ngáº­p nÄƒm n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>áº¾ thÃ¬ pháº£i bá» ra lÃ m taxi, Ä‘á»ƒ thÃ nh nghÄ©a Ä‘á»‹a ...</td>\n",
       "      <td>áº¾ thÃ¬ pháº£i bá» ra lÃ m taxi Ä‘á»ƒ thÃ nh nghÄ©a Ä‘á»‹a x...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>@ngocannguyen8049Â  VNCH bá»‹ Báº¯c Cá»™ng lá»«a kÃ½ vÃ ...</td>\n",
       "      <td>VNCH bá»‹ Báº¯c Cá»™ng lá»«a kÃ½ vÃ o báº£n hiá»‡p Ä‘á»‹nh Pari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CÃ¢u nÃ³i Ä‘á»ƒ Ä‘oi cá»§a Ã” PhÃºc \"cÃ¢y cá»™t Ä‘iá»‡n á»Ÿ Má»¹ c...</td>\n",
       "      <td>CÃ¢u nÃ³i Ä‘á»ƒ Ä‘oi cá»§a Ã” PhÃºc cÃ¢y cá»™t Ä‘iá»‡n á»Ÿ Má»¹ cÅ©...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ğŸ˜‚ cÃ³ mÃ  cho tiá»n chÃºng cÅ©ng khÃ´ng dÃ¡m</td>\n",
       "      <td>cÃ³ mÃ  cho tiá»n chÃºng cÅ©ng khÃ´ng dÃ¡m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LÃ  con ngÆ°á»i khÃ´ng ai láº¡i vÃ´ liÃªm sá»¹ tá»± lÃ m th...</td>\n",
       "      <td>LÃ  con ngÆ°á»i khÃ´ng ai láº¡i vÃ´ liÃªm sá»¹ tá»± lÃ m th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Má»™t biá»‡n phÃ¡p nghiá»‡p vá»¥ Ä‘Æ°á»£c Ä‘Ã o táº¡o ráº¥t ká»¹  r...</td>\n",
       "      <td>Má»™t biá»‡n phÃ¡p nghiá»‡p vá»¥ Ä‘Æ°á»£c Ä‘Ã o táº¡o ráº¥t ká»¹ rá»“...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Ong  ho im di may muon láº¥mo</td>\n",
       "      <td>Ong ho im di may muon láº¥mo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LÃºc nÃ o cÅ©ng dan do thÃ¡i vÃ  dct, nÃªn tÃ¬m hiá»ƒu ...</td>\n",
       "      <td>LÃºc nÃ o cÅ©ng dan do thÃ¡i vÃ  dct nÃªn tÃ¬m hiá»ƒu n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>@thanhracroi7177Â  \\nThá»© sÃ¡u lÃ  cÃ³ chiáº¿n tháº¯ng...</td>\n",
       "      <td>Thá»© sÃ¡u lÃ  cÃ³ chiáº¿n tháº¯ng rá»±c rá»¡ nhÆ° ngÃ y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ÄÃ¡nh cho Má»¹ cÃºt Ä‘Ã¡nh cho ngá»¥y nhÃ o thá»‘ng nháº¥t ...</td>\n",
       "      <td>ÄÃ¡nh cho Má»¹ cÃºt Ä‘Ã¡nh cho ngá»¥y nhÃ o thá»‘ng nháº¥t ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3/// vÃªn vÃ ng ko muá»‘n xem video nÃ y ğŸ˜‚ğŸ˜‚ğŸ˜‚</td>\n",
       "      <td>vÃªn vÃ ng ko muá»‘n xem video nÃ y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Cá»™ng Ä‘á»“ng chung váº­n má»‡nh tá»« Ä‘áº¹p nÃ y Táº­p Cáº­n BÃ¬...</td>\n",
       "      <td>Cá»™ng Ä‘á»“ng chung váº­n má»‡nh tá»« Ä‘áº¹p nÃ y Táº­p Cáº­n BÃ¬...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Dáº¡ pháº£i. â€œBÃ¡câ€ Ä‘Ã£ dáº¡y: â€œÄáº£ng viÃªn Ä‘i trÆ°á»›c, lÃ ...</td>\n",
       "      <td>Dáº¡ pháº£i BÃ¡c Ä‘Ã£ dáº¡y Äáº£ng viÃªn Ä‘i trÆ°á»›c lÃ ng nÆ°á»›...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Äá»«ng nÃ³i ngÃ´ ká»· háº¿t nÆ°á»›c cháº¥m khÃ´ng pháº£i bÃªÄ‘Ãª ...</td>\n",
       "      <td>Äá»«ng nÃ³i ngÃ´ ká»· háº¿t nÆ°á»›c cháº¥m khÃ´ng pháº£i bÃªÄ‘Ãª ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Giao Sim Táº¡i NhÃ  -  VÃ o TÃªn ChÃ­nh Chá»§\\nKiá»ƒm Tr...</td>\n",
       "      <td>Giao Sim Táº¡i NhÃ  VÃ o TÃªn ChÃ­nh Chá»§nKiá»ƒm Tra ÄÃº...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Há»™i Ä‘á»“ng báº£o an Hiá»‡p ChÃºng Quá»‘c</td>\n",
       "      <td>Há»™i Ä‘á»“ng báº£o an Hiá»‡p ChÃºng Quá»‘c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[TB] Táº·ng 20% giÃ¡ trá»‹ táº¥t cáº£ tháº» náº¡p trong ngÃ ...</td>\n",
       "      <td>TB Táº·ng giÃ¡ trá»‹ táº¥t cáº£ tháº» náº¡p trong ngÃ y Tiá»n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              content  \\\n",
       "0   Giá» Ä‘u cÃ´ng, chui thÃ¹ng láº¡nh thÃ´i. Äi XKLÄ cÅ©n...   \n",
       "1   Táº¬P Cáº¬N BÃŒNH QUA VIá»†T NAM THÄ‚M LÃ€ ÄIá»€U Ráº¤T Tá»T...   \n",
       "2   CÃ³ pháº£i â€œ chiÃªn anton â€œ lÃ  tháº±ng hÃ´m trÆ°á»›c vá» ...   \n",
       "3   Hay quÃ¡ ğŸŒ·ğŸ’™Buá»•i nÃ³i chuyá»‡n cá»§a 2 vá»‹ ráº¥t hay. Xi...   \n",
       "4   Sao khÃ´ng Ä‘Ã²i cÃ´ng lÃ½ cho náº¡n nhÃ¢n bá»‹ nhiá»…m ch...   \n",
       "5   quÃ¢n trÆ°Æ¡ng  phÃ¢n tiÌch  liÌ£ch sÆ°Ì‰ rÃ¢Ìt sÃ¢u sÄƒ...   \n",
       "6       Chxhcn viá»‡t nam (trá»« tá»± do, trá»« háº¡nh phÃºc)ğŸ˜‚ğŸ˜‚ğŸ˜‚   \n",
       "7                               Ngu váº­y sao giÃ u Ä‘Æ°á»£c   \n",
       "8   ÄÃ¢y cháº¯c lÃ  cÆ¡ há»™i cho Ä‘ang dÃ¢n chá»§ tháº±ng chiáº¿...   \n",
       "9                     Vin cÅ©ng biáº¿t táº­n dá»¥ng bÃ¡n hÃ ng   \n",
       "10  1,000 NGÆ¯á»œI Bá»Š Cáº¢NH SÃT Má»¸ Báº®N CHáº¾T Má»–I NÄ‚M CÃ’...   \n",
       "11  TT. BIden. Buot phai hannh dongg truoc khi nuo...   \n",
       "12  Chá»‘ng ngáº­p Ä‘Æ°á»ng nÃ³i 20 nÄƒm rá»“i mÃ  váº«n ngáº­p nÄƒ...   \n",
       "13  áº¾ thÃ¬ pháº£i bá» ra lÃ m taxi, Ä‘á»ƒ thÃ nh nghÄ©a Ä‘á»‹a ...   \n",
       "14  Â @ngocannguyen8049Â  VNCH bá»‹ Báº¯c Cá»™ng lá»«a kÃ½ vÃ ...   \n",
       "15  CÃ¢u nÃ³i Ä‘á»ƒ Ä‘oi cá»§a Ã” PhÃºc \"cÃ¢y cá»™t Ä‘iá»‡n á»Ÿ Má»¹ c...   \n",
       "16              ğŸ˜‚ cÃ³ mÃ  cho tiá»n chÃºng cÅ©ng khÃ´ng dÃ¡m   \n",
       "17  LÃ  con ngÆ°á»i khÃ´ng ai láº¡i vÃ´ liÃªm sá»¹ tá»± lÃ m th...   \n",
       "18  Má»™t biá»‡n phÃ¡p nghiá»‡p vá»¥ Ä‘Æ°á»£c Ä‘Ã o táº¡o ráº¥t ká»¹  r...   \n",
       "19                        Ong  ho im di may muon láº¥mo   \n",
       "20  LÃºc nÃ o cÅ©ng dan do thÃ¡i vÃ  dct, nÃªn tÃ¬m hiá»ƒu ...   \n",
       "21  Â @thanhracroi7177Â  \\nThá»© sÃ¡u lÃ  cÃ³ chiáº¿n tháº¯ng...   \n",
       "22  ÄÃ¡nh cho Má»¹ cÃºt Ä‘Ã¡nh cho ngá»¥y nhÃ o thá»‘ng nháº¥t ...   \n",
       "23            3/// vÃªn vÃ ng ko muá»‘n xem video nÃ y ğŸ˜‚ğŸ˜‚ğŸ˜‚   \n",
       "24  Cá»™ng Ä‘á»“ng chung váº­n má»‡nh tá»« Ä‘áº¹p nÃ y Táº­p Cáº­n BÃ¬...   \n",
       "25  Dáº¡ pháº£i. â€œBÃ¡câ€ Ä‘Ã£ dáº¡y: â€œÄáº£ng viÃªn Ä‘i trÆ°á»›c, lÃ ...   \n",
       "26  Äá»«ng nÃ³i ngÃ´ ká»· háº¿t nÆ°á»›c cháº¥m khÃ´ng pháº£i bÃªÄ‘Ãª ...   \n",
       "27  Giao Sim Táº¡i NhÃ  -  VÃ o TÃªn ChÃ­nh Chá»§\\nKiá»ƒm Tr...   \n",
       "28                    Há»™i Ä‘á»“ng báº£o an Hiá»‡p ChÃºng Quá»‘c   \n",
       "29  [TB] Táº·ng 20% giÃ¡ trá»‹ táº¥t cáº£ tháº» náº¡p trong ngÃ ...   \n",
       "\n",
       "                                      cleaned_content  \n",
       "0   Giá» Ä‘u cÃ´ng chui thÃ¹ng láº¡nh thÃ´i Äi XKLÄ cÅ©ng ...  \n",
       "1   Táº¬P Cáº¬N BÃŒNH QUA VIá»†T NAM THÄ‚M LÃ€ ÄIá»€U Ráº¤T Tá»T...  \n",
       "2   CÃ³ pháº£i chiÃªn anton lÃ  tháº±ng hÃ´m trÆ°á»›c vá» VN l...  \n",
       "3   Hay quÃ¡ Buá»•i nÃ³i chuyá»‡n cá»§a vá»‹ ráº¥t hay Xin cáº£m...  \n",
       "4   Sao khÃ´ng Ä‘Ã²i cÃ´ng lÃ½ cho náº¡n nhÃ¢n bá»‹ nhiá»…m ch...  \n",
       "5   quÃ¢n trÆ°Æ¡ng phÃ¢n tich lich sÆ° rÃ¢t sÃ¢u sÄƒc dÃª hiÃªu  \n",
       "6             Chxhcn viá»‡t nam trá»« tá»± do trá»« háº¡nh phÃºc  \n",
       "7                               Ngu váº­y sao giÃ u Ä‘Æ°á»£c  \n",
       "8   ÄÃ¢y cháº¯c lÃ  cÆ¡ há»™i cho Ä‘ang dÃ¢n chá»§ tháº±ng chiáº¿...  \n",
       "9                     Vin cÅ©ng biáº¿t táº­n dá»¥ng bÃ¡n hÃ ng  \n",
       "10  NGÆ¯á»œI Bá»Š Cáº¢NH SÃT Má»¸ Báº®N CHáº¾T Má»–I NÄ‚M CÃ’N KHÃ”N...  \n",
       "11  TT BIden Buot phai hannh dongg truoc khi nuoc ...  \n",
       "12  Chá»‘ng ngáº­p Ä‘Æ°á»ng nÃ³i nÄƒm rá»“i mÃ  váº«n ngáº­p nÄƒm n...  \n",
       "13  áº¾ thÃ¬ pháº£i bá» ra lÃ m taxi Ä‘á»ƒ thÃ nh nghÄ©a Ä‘á»‹a x...  \n",
       "14  VNCH bá»‹ Báº¯c Cá»™ng lá»«a kÃ½ vÃ o báº£n hiá»‡p Ä‘á»‹nh Pari...  \n",
       "15  CÃ¢u nÃ³i Ä‘á»ƒ Ä‘oi cá»§a Ã” PhÃºc cÃ¢y cá»™t Ä‘iá»‡n á»Ÿ Má»¹ cÅ©...  \n",
       "16                cÃ³ mÃ  cho tiá»n chÃºng cÅ©ng khÃ´ng dÃ¡m  \n",
       "17  LÃ  con ngÆ°á»i khÃ´ng ai láº¡i vÃ´ liÃªm sá»¹ tá»± lÃ m th...  \n",
       "18  Má»™t biá»‡n phÃ¡p nghiá»‡p vá»¥ Ä‘Æ°á»£c Ä‘Ã o táº¡o ráº¥t ká»¹ rá»“...  \n",
       "19                         Ong ho im di may muon láº¥mo  \n",
       "20  LÃºc nÃ o cÅ©ng dan do thÃ¡i vÃ  dct nÃªn tÃ¬m hiá»ƒu n...  \n",
       "21          Thá»© sÃ¡u lÃ  cÃ³ chiáº¿n tháº¯ng rá»±c rá»¡ nhÆ° ngÃ y  \n",
       "22  ÄÃ¡nh cho Má»¹ cÃºt Ä‘Ã¡nh cho ngá»¥y nhÃ o thá»‘ng nháº¥t ...  \n",
       "23                     vÃªn vÃ ng ko muá»‘n xem video nÃ y  \n",
       "24  Cá»™ng Ä‘á»“ng chung váº­n má»‡nh tá»« Ä‘áº¹p nÃ y Táº­p Cáº­n BÃ¬...  \n",
       "25  Dáº¡ pháº£i BÃ¡c Ä‘Ã£ dáº¡y Äáº£ng viÃªn Ä‘i trÆ°á»›c lÃ ng nÆ°á»›...  \n",
       "26  Äá»«ng nÃ³i ngÃ´ ká»· háº¿t nÆ°á»›c cháº¥m khÃ´ng pháº£i bÃªÄ‘Ãª ...  \n",
       "27  Giao Sim Táº¡i NhÃ  VÃ o TÃªn ChÃ­nh Chá»§nKiá»ƒm Tra ÄÃº...  \n",
       "28                    Há»™i Ä‘á»“ng báº£o an Hiá»‡p ChÃºng Quá»‘c  \n",
       "29  TB Táº·ng giÃ¡ trá»‹ táº¥t cáº£ tháº» náº¡p trong ngÃ y Tiá»n...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['cleaned_content'] = data['content'].apply(remove_control_and_normalize)\n",
    "\n",
    "# Hiá»ƒn thá»‹ má»™t sá»‘ dÃ²ng Ä‘áº§u Ä‘á»ƒ kiá»ƒm tra káº¿t quáº£\n",
    "data[['content', 'cleaned_content']].head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import models, pre_tokenizers, decoders, trainers, processors, Tokenizer, ByteLevelBPETokenizer\n",
    "\n",
    "# Initialize a tokenizer with BPE model\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "\n",
    "# Define vocabulary size for BPE\n",
    "vocab_size = 30000\n",
    "\n",
    "# Configure the BPE trainer\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=vocab_size, \n",
    "    min_frequency=2, \n",
    "    special_tokens=[\n",
    "        \"<s>\",  # Start of Sentence\n",
    "        \"<pad>\",  # Padding\n",
    "        \"</s>\",  # End of Sentence\n",
    "        \"<unk>\",  # Unknown Token\n",
    "        \"<mask>\",  # Masking Token\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Train the tokenizer using the cleaned content\n",
    "cleaned_content = data['cleaned_content']\n",
    "tokenizer.train_from_iterator(iterator=cleaned_content, trainer=trainer)\n",
    "tokenizer.save(\"tokenizer.json\")\n",
    "\n",
    "# Set up a RoBERTa-style post-processor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'T', 'ao', 'Ä lÃƒÅ‚', 'Ä bÃ¡Â»Ä³', 'Ä mÃƒÅ‚y', '</s>']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.post_processor = processors.RobertaProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "\n",
    "# Enable truncation to a maximum length of 512 tokens\n",
    "tokenizer.enable_truncation(max_length=512)\n",
    "\n",
    "# Save or return the tokenizer for further use\n",
    "\n",
    "encoding = tokenizer.encode(\"Tao lÃ  bá»‘ mÃ y!\")\n",
    "encoding.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tao lÃ  bá»‘ mÃ y'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "tokenizer.decode(encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element 1471 is invalid: None\n",
      "Element 1894 is invalid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: cleaned_content. If cleaned_content are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "c:\\Users\\Admin\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4797\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1800\n",
      "  Number of trainable parameters = 41209146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized content: [5775, 1209, 425, 2435, 4403, 2993, 468, 312, 204, 42, 43, 661, 324, 3567, 2539, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Tokenized content: [51, 466, 3322, 680, 420, 664, 511, 2092, 187, 896, 457, 758, 1204, 2447, 721, 507, 457, 846, 866, 664, 511, 3537, 1201, 3564, 587, 420, 664, 511, 721, 232, 231, 420, 260, 5994, 893, 1818, 420, 664, 511, 468, 613, 411, 231, 1023, 2192, 284, 461, 794, 2491, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Tokenized content: [2816, 334, 3871, 639, 3804, 187, 538, 1234, 698, 386, 560, 284, 1869, 516, 218, 370, 2017, 356, 498, 7480, 2134, 416, 6035, 559, 231, 723, 2929, 330, 906, 1482, 472, 1229, 187, 680, 869, 5124, 370, 6038, 115, 3226, 1725, 333, 5843, 562, 431, 521, 356, 1498, 260, 757, 512, 260, 351, 644, 2065, 456, 235, 7258, 56, 2309, 618, 803, 2000, 1584, 3448, 364, 7599, 2303, 305, 535, 1157, 829, 172, 1062, 490, 305, 1139, 163, 49, 4179, 1224, 187, 423, 216, 296, 279, 5118, 4613, 3892, 560, 412, 4203, 378, 1418, 336, 468, 476, 435, 4609, 618, 1693, 1890, 362, 218, 1016, 279, 383, 372, 174, 252, 2511, 219, 60, 45, 520, 3056, 1413, 433, 235, 893, 2419, 1426, 8161, 364, 1327, 1730, 427, 2196, 245, 272]\n",
      "Tokenized content: [5777, 445, 2929, 316, 791, 245, 935, 457, 443, 792, 908, 605, 935, 590, 429, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Tokenized content: [2822, 231, 1437, 425, 820, 238, 1193, 477, 370, 3019, 995, 974, 1195, 1121, 245, 587, 456, 664, 511, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff8ed2a9de5049af9358d2e97ed68f1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 94\u001b[0m\n\u001b[0;32m     86\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     87\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     88\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m     89\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_dataset,\n\u001b[0;32m     90\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[0;32m     91\u001b[0m )\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Thá»±c hiá»‡n pre-training mÃ´ hÃ¬nh\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:1501\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m   1498\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1499\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1500\u001b[0m )\n\u001b[1;32m-> 1501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:1749\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1747\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1748\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1749\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1752\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1753\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1754\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1755\u001b[0m ):\n\u001b[0;32m   1756\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1757\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:2508\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   2507\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2508\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   2511\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:2540\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2539\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2540\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   2541\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2542\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2543\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1107\u001b[0m, in \u001b[0;36mRobertaForMaskedLM.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1093\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroberta(\n\u001b[0;32m   1094\u001b[0m     input_ids,\n\u001b[0;32m   1095\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1104\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1105\u001b[0m )\n\u001b[0;32m   1106\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1107\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1109\u001b[0m masked_lm_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1144\u001b[0m, in \u001b[0;36mRobertaLMHead.forward\u001b[1;34m(self, features, **kwargs)\u001b[0m\n\u001b[0;32m   1141\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(x)\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;66;03m# project back to size of vocabulary with bias\u001b[39;00m\n\u001b[1;32m-> 1144\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import thÆ° viá»‡n cáº§n thiáº¿t\n",
    "from transformers import RobertaConfig, RobertaModel,RobertaForMaskedLM, RobertaTokenizerFast, DataCollatorForLanguageModeling, Trainer, TrainingArguments, AutoTokenizer, CONFIG_MAPPING\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Thiáº¿t láº­p cáº¥u hÃ¬nh mÃ´ hÃ¬nh vá»›i thÃ´ng sá»‘ tÃ¹y chá»‰nh\n",
    "model_type = 'roberta'  # Loáº¡i mÃ´ hÃ¬nh\n",
    "model_config = CONFIG_MAPPING[model_type]()  # Cáº¥u hÃ¬nh máº·c Ä‘á»‹nh cho mÃ´ hÃ¬nh\n",
    "override_config = {\n",
    "    \"num_hidden_layers\": 8,\n",
    "    \"num_attention_heads\": 8,\n",
    "    \"intermediate_size\": 2048,\n",
    "    \"hidden_size\": 512,\n",
    "    \"max_position_embeddings\": 130,  # 128 cho ná»™i dung + 2 cho token Ä‘áº·c biá»‡t\n",
    "}\n",
    "model_config.update(override_config)  # Cáº­p nháº­t cáº¥u hÃ¬nh tÃ¹y chá»‰nh\n",
    "\n",
    "# Khá»Ÿi táº¡o mÃ´ hÃ¬nh RoBERTa vá»›i cáº¥u hÃ¬nh tÃ¹y chá»‰nh\n",
    "model = RobertaForMaskedLM(model_config)\n",
    "\n",
    "# Sá»­ dá»¥ng tokenizer Ä‘Ã£ huáº¥n luyá»‡n tá»« tá»‡p hoáº·c thÆ° má»¥c\n",
    "tokenizer_path = \"C:\\\\Users\\\\Admin\\\\Downloads\\\\roberta\\\\mlm_bert_2\"  # ÄÆ°á»ng dáº«n Ä‘áº¿n thÆ° má»¥c chá»©a tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "# Sá»­ dá»¥ng bá»™ dá»¯ liá»‡u CSV Ä‘Ã£ cho\n",
    "dataset_path = \"cleaned_quarter_final.csv\"\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": dataset_path})\n",
    "train_dataset = dataset[\"train\"]\n",
    "for i in range(len(train_dataset)):\n",
    "    content = train_dataset[i].get(\"cleaned_content\", None)\n",
    "    \n",
    "    if content is None or not isinstance(content, str):\n",
    "        print(f\"Element {i} is invalid: {content}\")  # Ghi láº¡i pháº§n tá»­ lá»—i\n",
    "\n",
    "# HÃ m kiá»ƒm tra vÃ  chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u thÃ nh chuá»—i\n",
    "# HÃ m Ä‘á»ƒ Ä‘áº£m báº£o má»i giÃ¡ trá»‹ lÃ  chuá»—i vÃ  tráº£ vá» danh sÃ¡ch\n",
    "# for i in range(999 ,1005 ):\n",
    "#     content = train_dataset[i].get(\"cleaned_content\", \"\")\n",
    "#     if content is None:\n",
    "#         print(f\"Data {i} is None or empty.\")\n",
    "# Kiá»ƒm tra sau khi lá»c\n",
    "# Lá»c cÃ¡c pháº§n tá»­ cÃ³ giÃ¡ trá»‹ None trong \"cleaned_content\"\n",
    "valid_dataset = train_dataset.filter(lambda example: example.get(\"cleaned_content\") is not None)\n",
    "# Loáº¡i bá» cá»™t label\n",
    "dataset_without_label = valid_dataset.remove_columns(\"label\") \n",
    "# Hoáº·c tÃªn cá»™t liÃªn quan Ä‘áº¿n nhÃ£n\n",
    "dataset_without_label = dataset_without_label.remove_columns(\"content\")\n",
    "# Tokenize dá»¯ liá»‡u vÃ  thiáº¿t láº­p Data Collator cho Language Modeling\n",
    "def tokenize_function(example):\n",
    "    content = example.get(\"cleaned_content\", \"\")\n",
    "    \n",
    "    # Náº¿u lÃ  None hoáº·c khÃ´ng pháº£i lÃ  chuá»—i, chuyá»ƒn thÃ nh chuá»—i\n",
    "    if content is None or not isinstance(content, str):\n",
    "        content = \"\"\n",
    "\n",
    "    return tokenizer(example[\"cleaned_content\"], truncation=True, max_length=130, padding=\"max_length\")\n",
    "\n",
    "tokenized_dataset = dataset_without_label.map(tokenize_function, batched=True)\n",
    "# Láº¥y 5 pháº§n tá»­ Ä‘áº§u tiÃªn\n",
    "# Láº¥y 5 pháº§n tá»­ Ä‘áº§u tiÃªn tá»« táº­p dá»¯ liá»‡u\n",
    "first_5_examples = tokenized_dataset.select(range(5))\n",
    "\n",
    "for example in first_5_examples:\n",
    "    print(\"Tokenized content:\", example[\"input_ids\"])  # Kiá»ƒm tra ná»™i dung Ä‘Æ°á»£c token hÃ³a\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,  # Masked Language Modeling\n",
    "    mlm_probability=0.15  # Tá»· lá»‡ pháº§n trÄƒm cÃ¡c tá»« Ä‘Æ°á»£c mask\n",
    ")\n",
    "\n",
    "# Thiáº¿t láº­p cÃ¡c tham sá»‘ huáº¥n luyá»‡n\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=500,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500,\n",
    ")\n",
    "\n",
    "# Thiáº¿t láº­p Trainer vÃ  thá»±c hiá»‡n huáº¥n luyá»‡n\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Thá»±c hiá»‡n pre-training mÃ´ hÃ¬nh\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
